{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "170101049_Trigram_Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzTqUOCkH2_B"
      },
      "source": [
        "#Priyanshu Singh 170101049 Assignment 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbkS-_3OIP_u"
      },
      "source": [
        "###Downloading Corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZKRf_koG8Cs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59a18b4d-b469-4ada-f204-9884b3ff4713"
      },
      "source": [
        "!wget https://www.dropbox.com/s/1agrh5hdnkqd24c/en_wiki.txt?dl=0        # downloads the dataset\n",
        "!mv en_wiki.txt?dl=0 en_data.txt\n",
        "\n",
        "with open(\"en_data.txt\", \"r\") as file1:                                 # reading the data file\n",
        "    en_text = file1.read()\n",
        "en_text = en_text.replace('\\n', '')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-05 13:14:51--  https://www.dropbox.com/s/1agrh5hdnkqd24c/en_wiki.txt?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.9.1, 2620:100:601f:1::a27d:901\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.9.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/1agrh5hdnkqd24c/en_wiki.txt [following]\n",
            "--2020-11-05 13:14:51--  https://www.dropbox.com/s/raw/1agrh5hdnkqd24c/en_wiki.txt\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucd321d1a27d6bab23ff57f6e6c4.dl.dropboxusercontent.com/cd/0/inline/BCk8TWEeH-GDFio1QXMkcfiym2socx3MhP-2gJYLN3Exwl3CGqAHT-0LeEHR7Gjb6RaT4SIov2xm2TF0r1AxdlnCv26OVS-VNx1ltDCi6XZTIG7Ga9hICIgWgB_-IhE4VHA/file# [following]\n",
            "--2020-11-05 13:14:51--  https://ucd321d1a27d6bab23ff57f6e6c4.dl.dropboxusercontent.com/cd/0/inline/BCk8TWEeH-GDFio1QXMkcfiym2socx3MhP-2gJYLN3Exwl3CGqAHT-0LeEHR7Gjb6RaT4SIov2xm2TF0r1AxdlnCv26OVS-VNx1ltDCi6XZTIG7Ga9hICIgWgB_-IhE4VHA/file\n",
            "Resolving ucd321d1a27d6bab23ff57f6e6c4.dl.dropboxusercontent.com (ucd321d1a27d6bab23ff57f6e6c4.dl.dropboxusercontent.com)... 162.125.9.15, 2620:100:601d:15::a27d:50f\n",
            "Connecting to ucd321d1a27d6bab23ff57f6e6c4.dl.dropboxusercontent.com (ucd321d1a27d6bab23ff57f6e6c4.dl.dropboxusercontent.com)|162.125.9.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 103092525 (98M) [text/plain]\n",
            "Saving to: ‘en_wiki.txt?dl=0’\n",
            "\n",
            "en_wiki.txt?dl=0    100%[===================>]  98.32M  75.9MB/s    in 1.3s    \n",
            "\n",
            "2020-11-05 13:14:53 (75.9 MB/s) - ‘en_wiki.txt?dl=0’ saved [103092525/103092525]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kAAPO1HhVMQ"
      },
      "source": [
        "#Trigram Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "my1eCgpoIV07"
      },
      "source": [
        "###```Sentence Segmentation```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDbPVbIXHVNo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b6eeed7-9b88-4516-b92c-906cbf9a141a"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from IPython.display import display, Markdown\n",
        "import nltk\n",
        "nltk.download('punkt')  # in order to make \"nltk.tokenize.word_tokenize\" work\n",
        "\n",
        "# sentence segmentation using NLTK\n",
        "sent_tokens_en = nltk.sent_tokenize(en_text)    \n",
        "\n",
        "# addition of stop words and start words in the corpus\n",
        "start_str = \"START START \"\n",
        "end_str = \" STOP\"\n",
        "sent_tokens = []\n",
        "for sent in sent_tokens_en:\n",
        "  sent = sent.strip()\n",
        "  sent = start_str + sent + end_str\n",
        "  sent_tokens.append(sent)\n",
        "\n",
        "sent_tokens = sent_tokens[:2000]\n",
        "\n",
        "del sent_tokens_en"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV0S2q81Iblg"
      },
      "source": [
        "###```Preparation of Train+Dev and Test set```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TomR6QcJk1l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "8b690b17-276c-4a6e-a75f-aca86da3f299"
      },
      "source": [
        "import random\n",
        "from IPython.display import display, Markdown\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# splitting of data into train and test after random shuffling\n",
        "random.shuffle(sent_tokens)\n",
        "no_of_sentences = len(sent_tokens)\n",
        "train_data_len = 9 * (no_of_sentences // 10)\n",
        "train_sentences = sent_tokens[:train_data_len]\n",
        "test_sentences = sent_tokens[train_data_len:]\n",
        "\n",
        "# some statistics\n",
        "display(Markdown(\"#Sentence segmentation using NLTK\"))\n",
        "display(Markdown(\"##No of sentences after sentence segmentation are: {}\".format(len(sent_tokens))))\n",
        "display(Markdown(\"##No of sentences in training set: {}\".format(len(train_sentences))))\n",
        "display(Markdown(\"###Some of the train sentences are:\"))\n",
        "print(train_sentences[:20])\n",
        "display(Markdown(\"##No of sentences in test set: {}\".format(len(test_sentences))))\n",
        "display(Markdown(\"###Some of the test sentences are:\"))\n",
        "print(test_sentences[:20])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#Sentence segmentation using NLTK",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "##No of sentences after sentence segmentation are: 2000",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "##No of sentences in training set: 1800",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "###Some of the train sentences are:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[\"START START Janill Williams, a young athlete with much promise comes from Gray's Farm, Antigua. STOP\", 'START START The US Securities and Exchange Commission has accused the Antigua-based Stanford International Bank, owned by Texas billionaire Allen Stanford, of orchestrating a huge fraud which may have bilked investors of some $8 billion. STOP', 'START START Stars form from dense clouds in the ISM, and the evolutionary processes of stars result in the steady enrichment of the ISM with elements more massive than hydrogen and helium. STOP', 'START START Under this arrangement the author does not pay anything towards the expense of publication. STOP', \"START START Even the book review by the editors has more significance than the readership's reception.A standard contract for an author will usually include provision for payment in the form of an advance and royalties. STOP\", 'START START the set of \"theorems\" derived by it, seemed to be identical. STOP', 'START START The senior squad currently competes at Grupo I, the first division of Unión de Rugby de Buenos Aires league system.The club has ties with former association football club Alumni because both were established by Buenos Aires English High School students.The first club with the name \"Alumni\" played association football, having been found in 1898 by students of Buenos Aires English High School (BAEHS) along with director Alexander Watson Hutton. STOP', 'START START They have terpenoid essential oils which never contain iridoids.The oldest known fossils of members of Asteraceae are pollen grains from the Late Cretaceous of Antarctica, dated to ∼76–66 Mya (Campanian to Maastrichtian) and assigned to the extant genus \"Dasyphyllum\". STOP', 'START START Thus these spins cancel each other out, reducing the total magnetic dipole moment to zero in some atoms with even number of electrons.In ferromagnetic elements such as iron, cobalt and nickel, an odd number of electrons leads to an unpaired electron and a net overall magnetic moment. STOP', 'START START There are other psychiatric and medical problems that may mimic the symptoms of an anxiety disorder, such as hyperthyroidism.Common treatment options include lifestyle changes, therapy, and medications. STOP', 'START START Temperatures average , with a range from in the winter to in the summer and autumn. STOP', 'START START The gynoecium consists of two carpels fused into a single, bicarpellate pistil with an inferior ovary. STOP', 'START START The usual arrangement is for the actors to stand in an irregular line from one side of the screen to the other, with the actors at the end coming forward a little and standing more in profile than the others. STOP', 'START START A rift between Ditko and Lee developed, and the two men were not on speaking terms long before Ditko completed his last issue, \"The Amazing Spider-Man\" #38 (July 1966). STOP', \"START START They expected all the alpha particles to pass straight through with little deflection, because Thomson's model said that the charges in the atom are so diffuse that their electric fields could not affect the alpha particles much. STOP\", 'START START For Advanced Chemistry, hip hop is a “vehicle of general human emancipation,”. STOP', 'START START When an atom is in an external magnetic field, spectral lines become split into three or more components; a phenomenon called the Zeeman effect. STOP', 'START START For some people it is characterized by experiencing discomfort or awkwardness during physical social contact (e.g. STOP', 'START START Speer employed thousands of workers in two shifts. STOP', 'START START All six children performed in the normal range on most tests, including verbal IQ and performance IQ, but performed at least one standard deviation below age norms in at least one cognitive domain, such as complex attention (one child), short-term memory (one child) and internalizing behaviour/affect (two children). STOP']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "##No of sentences in test set: 200",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "###Some of the test sentences are:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "['START START The alternating regular writers were initially Dan Slott, Bob Gale, Marc Guggenheim, Fred Van Lente, and Zeb Wells, joined by a rotation of artists that included Chris Bachalo, Phil Jimenez, Mike McKone, John Romita Jr. and Marcos Martín. STOP', 'START START While almost everyone has experienced anxiety at some point in their lives, most do not develop long-term problems with anxiety.The behavioral effects of anxiety may include withdrawal from situations which have provoked anxiety in the past. STOP', 'START START The electron can change its state to a higher energy level by absorbing a photon with sufficient energy to boost it into the new quantum state. STOP', 'START START The neutron was discovered in 1932 by the English physicist James Chadwick.In the Standard Model of physics, electrons are truly elementary particles with no internal structure. STOP', 'START START Overall, the Earth is about 1.4% aluminium by mass (eighth in abundance by mass). STOP', 'START START He collaborated with his brother Kenneth and their articles appeared over the initials AKM. STOP', 'START START The most notable example has been the objection of many provinces of the communion (particularly in Africa and Asia) to the changing role of homosexuals in the North American churches (e.g., by blessing same-sex unions and ordaining and consecrating gays and lesbians in same-sex relationships) and to the process by which changes were undertaken. STOP', 'START START The voltage change is triphasic. STOP', 'START START Indeed, part of the Turkish ethnic minority of Frankfurt views Advanced Chemistry\\'s appeal to the German image as a \"symbolic betrayal of the right of ethnic minorities to \\'roots\\' or to any expression of cultural heritage.\" STOP', 'START START Some of the latter are studied in non-standard analysis.A deductive system consists of a set formula_39 of logical axioms, a set formula_40 of non-logical axioms, and a set formula_41 of \"rules of inference\". STOP', 'START START It has a crucial role in restricting axonal regeneration in adult mammalian central nervous system. STOP', 'START START According to Cutting, social phobics do not fear the crowd but the fact that they may be judged negatively.Social anxiety varies in degree and severity. STOP', 'START START It was used to write the Aramaic language, and had displaced the Paleo-Hebrew alphabet for the writing of Hebrew. STOP', 'START START Hence electrons bound near the center of the potential well require more energy to escape than those at greater separations.Electrons, like other particles, have properties of both a particle and a wave. STOP', 'START START The coolest period is between December and February. STOP', 'START START Native aluminium metal can only be found as a minor phase in low oxygen fugacity environments, such as the interiors of certain volcanoes. STOP', 'START START \"The World of Pooh\" won the Lewis Carroll Shelf Award in 1958.The success of his children\\'s books was to become a source of considerable annoyance to Milne, whose self-avowed aim was to write whatever he pleased and who had, until then, found a ready audience for each change of direction: he had freed pre-war \"Punch\" from its ponderous facetiousness; he had made a considerable reputation as a playwright (like his idol J. M. Barrie) on both sides of the Atlantic; he had produced a witty piece of detective writing in \"The Red House Mystery\" (although this was severely criticised by Raymond Chandler for the implausibility of its plot). STOP', 'START START It is estimated to be the 14th most common element in the Universe, by mass-fraction. STOP', 'START START They live in the Bel Air section of Los Angeles, California, just north of Sunset Boulevard.The couple’s only child, Karen Toffler, (1954–2000), died at the age of 46 after more than a decade suffering from Guillain–Barré syndrome.Alvin Toffler was born in 1928 in New York City, the son of Rose (Albaum) and Sam Toffler. STOP', 'START START In 2013 about 10% of the domestic shipments in the United States were used for other applications. STOP']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wesBlDCaJm-9"
      },
      "source": [
        "# Helper functions to for n-grams analysis\n",
        "from collections import defaultdict\n",
        "\n",
        "# returns a list of ngrams\n",
        "def extract_ngrams(tokens, num):\n",
        "    n_grams = ngrams(tokens, num)\n",
        "    return [ ' '.join(grams) for grams in n_grams]\n",
        "\n",
        "# returns a dict of ngrams\n",
        "def get_frequency(tokens,num):\n",
        "  n_grams = extract_ngrams(tokens,num)\n",
        "  freq = defaultdict(int)\n",
        "  for i in n_grams:\n",
        "    freq[i] = freq.get(i,0) + 1\n",
        "  return freq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nS6t4sBcIiiI"
      },
      "source": [
        "##```Linear Interpolation```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REC-y55tKHeR"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# returns the probability of a sentence as calculated by a trigram markov model with linear interpolation\n",
        "def prob_sentence_interpolation(tokens, lambda1, lambda2, lambda3):\n",
        "  \n",
        "  modified_tokens = []\n",
        "  \n",
        "  # replace less frequent words with \"UNK\" token\n",
        "  for token in tokens:\n",
        "    if freq_unigrams.get(token, 0) < 5:\n",
        "      token = \"UNK\"\n",
        "    modified_tokens.append(token)\n",
        "\n",
        "  # to avoid underflow, we calculate log probabilities\n",
        "  log_prob = 0\n",
        "  for i, val in enumerate(modified_tokens):\n",
        "    if i > 1:\n",
        "      str1 = modified_tokens[i-2] + \" \" + modified_tokens[i-1] + \" \" + modified_tokens[i]               # (u,v,w) trigram\n",
        "      str2 = modified_tokens[i-1] + \" \" + modified_tokens[i]                                            # (v,w) bigram\n",
        "      cnt1 = freq_trigrams.get(str1, 0)\n",
        "      cnt2 = freq_bigrams.get(str2, 1)\n",
        "      cnt3 = freq_unigrams.get(modified_tokens[i-1], 1)\n",
        "      cnt4 = freq_unigrams.get(modified_tokens[i], 0)\n",
        "      linear_inter = lambda1 * (cnt1 / cnt2) + lambda2 * (cnt2 / cnt3) + lambda3 * (cnt4 / len(modified_train_tokens))\n",
        "      if linear_inter == 0:\n",
        "        continue\n",
        "      max_likelihood_prob = np.log(linear_inter)\n",
        "      log_prob += max_likelihood_prob\n",
        "\n",
        "  return log_prob\n",
        "\n",
        "# calculates the likelihood value which is used for finding optimal lambda's for linear interpolation\n",
        "# Likelihood value -> summation(c'(u,v,w) * log(lambda1*prob_trigram + lambda2*prob_bigram + lambda3*prob_unigram)) where c'(u,v,w) is count in dev set\n",
        "def calc_likelihood_interpolation(lambda1, lambda2, lambda3):\n",
        "  ans = 0\n",
        "  for key, value in freq_trigrams.items():\n",
        "    res = key.split()\n",
        "    val = freq_trigrams_dev.get(key, 0)\n",
        "    str1 = res[0] + \" \" + res[1]\n",
        "    val1 = lambda1 * (value / freq_bigrams.get(str1, 1))\n",
        "    val2 = lambda2 * (freq_bigrams.get(str1, 0) / freq_unigrams.get(res[1], 1))\n",
        "    val3 = lambda3 * (freq_unigrams.get(res[2], 0) / len(modified_train_tokens))\n",
        "    ans += np.log(val1 + val2 + val3) * val\n",
        "\n",
        "  return ans\n",
        "\n",
        "# returns the perplexity over Dev set by doing a grid search for optimal lambda calculation\n",
        "def calc_perplexity_interpolation(sents):\n",
        "  m = len(sents)\n",
        "  n = 0\n",
        "  best_lambda1 = 0\n",
        "  best_lambda2 = 0\n",
        "  best_lambda3 = 0\n",
        "  max_log_likelihood = -100000000000\n",
        "\n",
        "  # doing a grid search for lambda, starting from 0.1 to 1.0 with steps of size 0.1\n",
        "  for lambda1 in range(1, 11):\n",
        "    for lambda2 in range(1, 11):\n",
        "      lambda3 = 10 - lambda1 - lambda2\n",
        "      lambda1 = lambda1 / 10\n",
        "      lambda2 = lambda2 / 10\n",
        "      lambda3 = lambda3 / 10\n",
        "      if lambda3 >= 0:\n",
        "        curr_log_likelihood = calc_likelihood_interpolation(lambda1, lambda2, lambda3)\n",
        "        if max_log_likelihood < curr_log_likelihood:\n",
        "          max_log_likelihood = curr_log_likelihood\n",
        "          best_lambda1 = lambda1\n",
        "          best_lambda2 = lambda2\n",
        "          best_lambda3 = lambda3\n",
        "  \n",
        "  # Calculation of perplexity for the whole test corpus\n",
        "  # Reference:- https://stats.stackexchange.com/a/143638\n",
        "  prob_sum = 0\n",
        "  for sent in sents:\n",
        "    tokens = nltk.word_tokenize(sent)\n",
        "    n += len(tokens)\n",
        "    prob_sum += prob_sentence_interpolation(tokens, best_lambda1, best_lambda2, best_lambda3)\n",
        "  \n",
        "  ans = -prob_sum / n\n",
        "  ans = pow(math.e, ans)\n",
        "  return ans, best_lambda1, best_lambda2, best_lambda3, max_log_likelihood"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxvwAM3NIotI"
      },
      "source": [
        "##```Discounting n-grams```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GPi3-7fLdtg"
      },
      "source": [
        "# returns the discounted probabilities of bigram tokens as discussed in slides\n",
        "def discountBigrams(sents, beta):\n",
        "  discounted_probs = defaultdict(int)\n",
        "\n",
        "  for key, val in freq_unigrams.items():\n",
        "    missing_mass = 0\n",
        "    cum_weight = 0\n",
        "\n",
        "    for token in freq_unigrams.keys():\n",
        "      new_key = key + \" \" + token\n",
        "      if freq_bigrams.get(new_key, 0) > 0:\n",
        "        temp = freq_bigrams[new_key] - beta\n",
        "        missing_mass += temp / val\n",
        "        discounted_probs[new_key] = temp / val\n",
        "      else:\n",
        "        cnt = freq_unigrams.get(token, 0)\n",
        "        cum_weight += cnt\n",
        "      \n",
        "    missing_mass = 1 - missing_mass\n",
        "    for token in freq_unigrams.keys():\n",
        "      new_key = key + \" \" + token\n",
        "      if freq_bigrams.get(new_key, 0) == 0:\n",
        "        cnt = freq_unigrams.get(token, 0)\n",
        "        discounted_probs[new_key] = missing_mass * (cnt / cum_weight)\n",
        "\n",
        "  return discounted_probs \n",
        "\n",
        "# returns the discounted probabilities of trigram tokens using discounted bigram probabilities as discussed in slides\n",
        "def discountTrigrams(sents, beta, discounted_bigrams_prob):\n",
        "  discounted_probs = defaultdict(int)\n",
        "\n",
        "  for key, val in freq_bigrams.items():\n",
        "    missing_mass = 0\n",
        "    cum_weight = 0\n",
        "\n",
        "    for token in freq_unigrams.keys():\n",
        "      new_key = key + \" \" + token\n",
        "      if freq_trigrams.get(new_key, 0) > 0:\n",
        "        temp = freq_trigrams[new_key] - beta\n",
        "        missing_mass += temp / val\n",
        "        discounted_probs[new_key] = temp / val\n",
        "      else:\n",
        "        res = key.split()\n",
        "        temp = res[1]\n",
        "        temp = temp + \" \" + token\n",
        "        cnt = discounted_bigrams_prob.get(temp, 0)\n",
        "        cum_weight += cnt\n",
        "    \n",
        "    missing_mass = 1 - missing_mass\n",
        "    for token in freq_unigrams.keys():\n",
        "      new_key = key + \" \" + token\n",
        "      if freq_trigrams.get(new_key, 0) == 0:\n",
        "        res = key.split()\n",
        "        temp = res[1]\n",
        "        temp = temp + \" \" + token\n",
        "        cnt = discounted_bigrams_prob.get(temp, 0)\n",
        "        discounted_probs[new_key] = missing_mass * (cnt / cum_weight)\n",
        "  \n",
        "  return discounted_probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLlHvTyniAVe"
      },
      "source": [
        "##```Perplexity and likelihood calculations for discounting```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jckjkz5xLv-V"
      },
      "source": [
        "# returns the probability of a sentence as calculated by a trigram markov model with discounting\n",
        "def prob_sentence_discount(tokens, prob_discount_trigrams):\n",
        "  modified_tokens = []\n",
        "\n",
        "  # replace less frequent words with \"UNK\" token\n",
        "  for token in tokens:\n",
        "    if freq_unigrams.get(token, 0) < 5:\n",
        "      token = \"UNK\"\n",
        "    modified_tokens.append(token)\n",
        "\n",
        "  log_prob = 0\n",
        "\n",
        "  for i, val in enumerate(modified_tokens):\n",
        "    if i > 1:\n",
        "      str1 = modified_tokens[i-2] + \" \" + modified_tokens[i-1] + \" \" + modified_tokens[i]\n",
        "      cnt = prob_discount_trigrams.get(str1, 0)\n",
        "      if cnt == 0:\n",
        "        continue\n",
        "      log_prob += np.log(cnt)\n",
        "  \n",
        "  return log_prob\n",
        "\n",
        "# calculates the likelihood on dev set for finding optimal beta for discounting bigrams\n",
        "def calc_likelihood_discount_bigrams(discounted_bigrams_prob):\n",
        "  ans = 0\n",
        "  for key, val in freq_bigrams_dev.items():\n",
        "      ans += val * np.log(discounted_bigrams_prob.get(key, 1))\n",
        "  return ans\n",
        "\n",
        "# calculates the likelihood on dev set for finding optimal beta for discounting trigrams\n",
        "def calc_likelihood_discount_trigrams(discounted_trigrams_prob):\n",
        "  ans = 0\n",
        "  for key, val in freq_trigrams_dev.items():\n",
        "      ans += val * np.log(discounted_trigrams_prob.get(key, 1))\n",
        "  return ans\n",
        "\n",
        "# returns the perplexity over Dev set by doing a grid search for optimal beta calculation\n",
        "def calc_perplexity_discount(sents, modified_train_tokens):\n",
        "  m = len(sents)        \n",
        "  n = 0                 \n",
        "  best_beta_bigram = 0\n",
        "  best_beta_trigram = 0\n",
        "  max_log_likelihood1 = -100000000000\n",
        "  max_log_likelihood2 = -100000000000\n",
        "\n",
        "  # doing a grid search for beta of bigram, starting from 0.1 to 1.0 with steps of size 0.1\n",
        "  for beta1 in range(1, 11):\n",
        "    beta1 = beta1 / 10\n",
        "    prob_discount_bigrams = discountBigrams(modified_train_tokens, beta1)\n",
        "    curr_log_likelihood = calc_likelihood_discount_bigrams(prob_discount_bigrams)\n",
        "    if max_log_likelihood1 < curr_log_likelihood:\n",
        "      max_log_likelihood1 = curr_log_likelihood\n",
        "      best_beta_bigram = beta1\n",
        "    del prob_discount_bigrams\n",
        "  \n",
        "  prob_discount_bigrams = discountBigrams(modified_train_tokens, best_beta_bigram)\n",
        "\n",
        "  # doing a grid search for beta of trigram, starting from 0.1 to 1.0 with steps of size 0.1\n",
        "  for beta2 in range(1, 11):\n",
        "    beta2 = beta2 / 10\n",
        "    prob_discount_trigrams = discountTrigrams(modified_train_tokens, beta2, prob_discount_bigrams)\n",
        "    curr_log_likelihood = calc_likelihood_discount_trigrams(prob_discount_trigrams)\n",
        "\n",
        "    if max_log_likelihood2 < curr_log_likelihood:\n",
        "      max_log_likelihood2 = curr_log_likelihood\n",
        "      best_beta_trigram = beta2\n",
        "    del prob_discount_trigrams\n",
        "\n",
        "  prob_discount_trigrams = discountTrigrams(modified_train_tokens, best_beta_trigram, prob_discount_bigrams)\n",
        "\n",
        "  # Calculation of perplexity for the whole test corpus\n",
        "  # Reference:- https://stats.stackexchange.com/a/143638\n",
        "  prob_sum = 0\n",
        "  for sent in sents:\n",
        "    tokens = nltk.word_tokenize(sent)\n",
        "    n += len(tokens)\n",
        "    prob_sum += prob_sentence_discount(tokens, prob_discount_trigrams)\n",
        "  \n",
        "  ans = -prob_sum / n\n",
        "  ans = pow(math.e, ans)\n",
        "  return ans, best_beta_bigram, best_beta_trigram, max_log_likelihood2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeoboJBdhPSg"
      },
      "source": [
        "##```Laplace Smoothing```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyuwNXoFcT6N"
      },
      "source": [
        "# calculates probability of trigrams in training set from the bigram and trigram frequencies of training set\n",
        "def laplace_probabilities(freq_bigrams, freq_trigrams, total_words):\n",
        "  laplace_prob = {}\n",
        "  for key, val in freq_trigrams.items():\n",
        "    res = key.split()\n",
        "    temp = res[0] + \" \" + res[1]\n",
        "    prob = (val + 1) / (freq_bigrams.get(temp) + total_words)\n",
        "    laplace_prob[key] = prob\n",
        "\n",
        "  return laplace_prob\n",
        "\n",
        "# calculates the log probability(to prevent underflow) of a sentence by trigram model\n",
        "def prob_sentence_laplace(tokens, laplace_prob):\n",
        "  modified_tokens = []\n",
        "\n",
        "  # replace less frequent words with \"UNK\" token\n",
        "  for token in tokens:\n",
        "    if freq_unigrams.get(token, 0) < 5:\n",
        "      token = \"UNK\"\n",
        "    modified_tokens.append(token)\n",
        "\n",
        "  log_prob = 0\n",
        "\n",
        "  for i, val in enumerate(modified_tokens):\n",
        "    if i > 1:\n",
        "      str1 = modified_tokens[i-2] + \" \" + modified_tokens[i-1] + \" \" + modified_tokens[i]\n",
        "      cnt = laplace_prob.get(str1, 0)\n",
        "      if cnt == 0:\n",
        "        cnt = 1 / len(modified_tokens)\n",
        "      log_prob += np.log(cnt)\n",
        "  \n",
        "  return log_prob\n",
        "\n",
        "# calculates the perplexity on passed corpus\n",
        "# Reference:- https://stats.stackexchange.com/a/143638\n",
        "def calc_perplexity_laplace(sents, laplace_prob):\n",
        "  m = len(sents)\n",
        "  n = 0\n",
        "  prob_sum = 0\n",
        "  for sent in sents:\n",
        "    tokens = nltk.word_tokenize(sent)\n",
        "    n += len(tokens)\n",
        "    prob_sum += prob_sentence_laplace(tokens, laplace_prob)\n",
        "  \n",
        "  ans = -prob_sum / n\n",
        "  ans = pow(math.e, ans)\n",
        "  return ans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w13aS2mgIyMX"
      },
      "source": [
        "##```Functions to calculate perplexity on test set```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpxvOqyb0jbG"
      },
      "source": [
        "# calculates the perplexity on test set using linear interpolation\n",
        "def test_perplexity_interpolation(sents, lambda1, lambda2, lambda3):\n",
        "  m = len(sents)        \n",
        "  n = 0\n",
        "  prob_sum = 0\n",
        "  for sent in sents:\n",
        "    tokens = nltk.word_tokenize(sent)\n",
        "    n += len(tokens)\n",
        "    prob_sum += prob_sentence_interpolation(tokens, lambda1, lambda2, lambda3)\n",
        "  \n",
        "  ans = -prob_sum / n\n",
        "  ans = pow(math.e, ans)\n",
        "  return ans\n",
        "\n",
        "# calculates the perplexity on test set using discounting\n",
        "def test_perplexity_discount(sents, modified_train_tokens, best_beta_bigram, best_beta_trigram):\n",
        "  m = len(sents)        \n",
        "  n = 0\n",
        "  prob_discount_bigrams = discountBigrams(modified_train_tokens, best_beta_bigram)\n",
        "  prob_discount_trigrams = discountTrigrams(modified_train_tokens, best_beta_trigram, prob_discount_bigrams)\n",
        "\n",
        "  prob_sum = 0\n",
        "  for sent in sents:\n",
        "    tokens = nltk.word_tokenize(sent)\n",
        "    n += len(tokens)\n",
        "    prob_sum += prob_sentence_discount(tokens, prob_discount_trigrams)\n",
        "  \n",
        "  ans = -prob_sum / n\n",
        "  ans = pow(math.e, ans)\n",
        "  return ans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qIoTEMBI4kd"
      },
      "source": [
        "##```Results```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6e-OXViPqve",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1d94e580-d92f-4332-8d7e-fc405b055af1"
      },
      "source": [
        "from nltk.util import ngrams\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "# making 5 different train and dev sets, finding optimal hyperparameters of trigram model and then evaluating the test set for perplexity\n",
        "for i in range(5):\n",
        "  temp = train_sentences\n",
        "  random.shuffle(train_sentences)                             # random shuffling\n",
        "  no_of_sentences = len(train_sentences)\n",
        "  train_data_len = 9 * (no_of_sentences // 10)                # 90:10 split for training and dev set\n",
        "  dev_sentences = train_sentences[train_data_len:]\n",
        "  train_sentences = train_sentences[:train_data_len]\n",
        "\n",
        "  if i == 0:\n",
        "    display(Markdown(\"##No of sentences in training set: {}\".format(len(train_sentences))))\n",
        "    display(Markdown(\"##No of sentences in Dev set: {}\".format(len(dev_sentences))))\n",
        "\n",
        "  # generating the training text from training sentences\n",
        "  train_text = \"\"\n",
        "  for sent in train_sentences:\n",
        "    train_text += sent\n",
        "    train_text += \"\\n\"\n",
        "  train_text = train_text[:-1]\n",
        "\n",
        "  # generating the dev text from dev sentences\n",
        "  dev_text = \"\"\n",
        "  for sent in dev_sentences:\n",
        "    dev_text += sent\n",
        "    dev_text += \"\\n\"\n",
        "  dev_text = dev_text[:-1]\n",
        "\n",
        "  # generating train and dev tokens\n",
        "  train_tokens = nltk.word_tokenize(train_text)\n",
        "  dev_tokens = nltk.word_tokenize(dev_text)\n",
        "  freq_unigrams = get_frequency(train_tokens, 1)\n",
        "\n",
        "  # replacing less frequent words with <UNK> token in training set\n",
        "  modified_train_tokens = []\n",
        "  for token in train_tokens:\n",
        "    if freq_unigrams.get(token, 0) < 5:\n",
        "      token = \"UNK\"\n",
        "    modified_train_tokens.append(token)\n",
        "\n",
        "  # replacing less frequent words with <UNK> token in dev set\n",
        "  modified_dev_tokens = []\n",
        "  for token in dev_tokens:\n",
        "    if freq_unigrams.get(token, 0) < 5:\n",
        "      token = \"UNK\"\n",
        "    modified_dev_tokens.append(token)\n",
        "\n",
        "  del freq_unigrams\n",
        "\n",
        "  # generating frequency dict of n-grams from train set and dev set\n",
        "  freq_unigrams = get_frequency(modified_train_tokens, 1)\n",
        "  freq_bigrams = get_frequency(modified_train_tokens, 2)\n",
        "  freq_trigrams = get_frequency(modified_train_tokens, 3)\n",
        "  freq_bigrams_dev = get_frequency(modified_dev_tokens, 2)\n",
        "  freq_trigrams_dev = get_frequency(modified_dev_tokens, 3)\n",
        "  laplace_prob = laplace_probabilities(freq_bigrams, freq_trigrams, len(modified_train_tokens))\n",
        "\n",
        "  # various perplexity calculations on dev set\n",
        "  perplexity_with_interpolation, best_lambda1, best_lambda2, best_lambda3, L_max_interpolation = calc_perplexity_interpolation(dev_sentences)\n",
        "  perplexity_with_discount, best_beta_bigram, best_beta_trigram, L_max_discount = calc_perplexity_discount(dev_sentences, modified_train_tokens)\n",
        "  perplexity_with_laplace = calc_perplexity_laplace(dev_sentences, laplace_prob)\n",
        "\n",
        "  print('\\033[90m' + \"###### Iteration\", i+1, \"######\" + '\\033[0m')\n",
        "  print('\\033[93m' + \"On development set\" + '\\033[0m')\n",
        "  print(\"------------------------- Linear Interpolation ---------------------------\")\n",
        "  print(\"Optimal log likelihood : \", L_max_interpolation)\n",
        "  print(\"Optimal lambdas : \", best_lambda1, best_lambda2, best_lambda3)\n",
        "  print(\"Perplexity : \", perplexity_with_interpolation)\n",
        "\n",
        "  print(\"------------------------- Discounting ---------------------------\")\n",
        "  print(\"Optimal log likelihood : \", L_max_discount)\n",
        "  print(\"Optimal beta for bigram discounting: \", best_beta_bigram)\n",
        "  print(\"Optimal beta for trigram discounting: \", best_beta_trigram)\n",
        "  print(\"Perplexity : \", perplexity_with_discount)\n",
        "\n",
        "  print(\"------------------------- Laplace smoothing ---------------------------\")\n",
        "  print(\"Perplexity : \", perplexity_with_laplace)\n",
        "  print(\"\\n\")\n",
        "\n",
        "  # perplexity calculation on test set\n",
        "  perplexity_test_interpolation = test_perplexity_interpolation(test_sentences, best_lambda1, best_lambda2, best_lambda3)\n",
        "  perplexity_test_discount = test_perplexity_discount(test_sentences, modified_train_tokens, best_beta_bigram, best_beta_trigram)\n",
        "  perplexity_test_laplace = calc_perplexity_laplace(test_sentences, laplace_prob)\n",
        "\n",
        "  print('\\033[93m' + \"On test set\" + '\\033[0m')\n",
        "  print(\"Peplexity with Linear Interpolation : \", perplexity_test_interpolation)\n",
        "  print(\"Peplexity with Discounting: \", perplexity_test_discount)\n",
        "  print(\"Peplexity with Laplace Smoothing: \", perplexity_test_laplace)\n",
        "  print(\"\\n\\n\")\n",
        "\n",
        "  train_sentences = temp\n",
        "\n",
        "  del freq_unigrams\n",
        "  del freq_bigrams\n",
        "  del freq_trigrams\n",
        "  del freq_trigrams_dev\n",
        "  del freq_bigrams_dev\n",
        "  del train_text\n",
        "  del dev_text\n",
        "  del modified_train_tokens\n",
        "  del modified_dev_tokens\n",
        "  del train_tokens\n",
        "  del dev_tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "##No of sentences in training set: 1620",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "##No of sentences in Dev set: 180",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[90m###### Iteration 1 ######\u001b[0m\n",
            "\u001b[93mOn development set\u001b[0m\n",
            "------------------------- Linear Interpolation ---------------------------\n",
            "Optimal log likelihood :  -6193.2823359387185\n",
            "Optimal lambdas :  0.9 0.1 0.0\n",
            "Perplexity :  36.42104929798983\n",
            "------------------------- Discounting ---------------------------\n",
            "Optimal log likelihood :  -20632.57895309312\n",
            "Optimal beta for bigram discounting:  0.6\n",
            "Optimal beta for trigram discounting:  0.7\n",
            "Perplexity :  20.652984502166873\n",
            "------------------------- Laplace smoothing ---------------------------\n",
            "Perplexity :  339.1967090168631\n",
            "\n",
            "\n",
            "\u001b[93mOn test set\u001b[0m\n",
            "Peplexity with Linear Interpolation :  34.27061526816665\n",
            "Peplexity with Discounting:  20.19779842093701\n",
            "Peplexity with Laplace Smoothing:  354.0467516907671\n",
            "\n",
            "\n",
            "\n",
            "\u001b[90m###### Iteration 2 ######\u001b[0m\n",
            "\u001b[93mOn development set\u001b[0m\n",
            "------------------------- Linear Interpolation ---------------------------\n",
            "Optimal log likelihood :  -6291.9077446398605\n",
            "Optimal lambdas :  0.9 0.1 0.0\n",
            "Perplexity :  36.984938329389394\n",
            "------------------------- Discounting ---------------------------\n",
            "Optimal log likelihood :  -21058.15508191909\n",
            "Optimal beta for bigram discounting:  0.6\n",
            "Optimal beta for trigram discounting:  0.7\n",
            "Perplexity :  21.4475111067963\n",
            "------------------------- Laplace smoothing ---------------------------\n",
            "Perplexity :  345.9746285786821\n",
            "\n",
            "\n",
            "\u001b[93mOn test set\u001b[0m\n",
            "Peplexity with Linear Interpolation :  33.98804106256074\n",
            "Peplexity with Discounting:  20.170605687853683\n",
            "Peplexity with Laplace Smoothing:  351.99476922492875\n",
            "\n",
            "\n",
            "\n",
            "\u001b[90m###### Iteration 3 ######\u001b[0m\n",
            "\u001b[93mOn development set\u001b[0m\n",
            "------------------------- Linear Interpolation ---------------------------\n",
            "Optimal log likelihood :  -6550.989728969394\n",
            "Optimal lambdas :  0.9 0.1 0.0\n",
            "Perplexity :  36.71874645904502\n",
            "------------------------- Discounting ---------------------------\n",
            "Optimal log likelihood :  -21470.001522549734\n",
            "Optimal beta for bigram discounting:  0.6\n",
            "Optimal beta for trigram discounting:  0.7\n",
            "Perplexity :  21.023473706207938\n",
            "------------------------- Laplace smoothing ---------------------------\n",
            "Perplexity :  369.6281170185677\n",
            "\n",
            "\n",
            "\u001b[93mOn test set\u001b[0m\n",
            "Peplexity with Linear Interpolation :  34.245044232320204\n",
            "Peplexity with Discounting:  19.995720279638796\n",
            "Peplexity with Laplace Smoothing:  349.5210743272877\n",
            "\n",
            "\n",
            "\n",
            "\u001b[90m###### Iteration 4 ######\u001b[0m\n",
            "\u001b[93mOn development set\u001b[0m\n",
            "------------------------- Linear Interpolation ---------------------------\n",
            "Optimal log likelihood :  -6746.47749787866\n",
            "Optimal lambdas :  0.9 0.1 0.0\n",
            "Perplexity :  38.76760857305028\n",
            "------------------------- Discounting ---------------------------\n",
            "Optimal log likelihood :  -22187.191624536805\n",
            "Optimal beta for bigram discounting:  0.6\n",
            "Optimal beta for trigram discounting:  0.7\n",
            "Perplexity :  21.613618032822355\n",
            "------------------------- Laplace smoothing ---------------------------\n",
            "Perplexity :  347.6687329396406\n",
            "\n",
            "\n",
            "\u001b[93mOn test set\u001b[0m\n",
            "Peplexity with Linear Interpolation :  33.65781668648138\n",
            "Peplexity with Discounting:  19.975505538829818\n",
            "Peplexity with Laplace Smoothing:  350.7007242175554\n",
            "\n",
            "\n",
            "\n",
            "\u001b[90m###### Iteration 5 ######\u001b[0m\n",
            "\u001b[93mOn development set\u001b[0m\n",
            "------------------------- Linear Interpolation ---------------------------\n",
            "Optimal log likelihood :  -6010.907226635516\n",
            "Optimal lambdas :  0.9 0.1 0.0\n",
            "Perplexity :  36.20205905353426\n",
            "------------------------- Discounting ---------------------------\n",
            "Optimal log likelihood :  -19593.312446101467\n",
            "Optimal beta for bigram discounting:  0.6\n",
            "Optimal beta for trigram discounting:  0.7\n",
            "Perplexity :  21.235459024049053\n",
            "------------------------- Laplace smoothing ---------------------------\n",
            "Perplexity :  344.6862457700158\n",
            "\n",
            "\n",
            "\u001b[93mOn test set\u001b[0m\n",
            "Peplexity with Linear Interpolation :  34.49185573486727\n",
            "Peplexity with Discounting:  20.08182102104323\n",
            "Peplexity with Laplace Smoothing:  352.784857362685\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}